#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.monitor import Monitor
from kinova_env import KinovaEnv
from datetime import datetime
import torch
import random
import numpy as np

SEED = 30

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False


def make_env():
    env = KinovaEnv()
    env = Monitor(env)  # ç”¨äºè®°å½•æ¯ä¸ª episode çš„å›æŠ¥å’Œé•¿åº¦
    env.reset(seed=SEED)
    return env


def main():
    # åˆ›å»ºä¿å­˜è·¯å¾„
    log_dir = os.path.join("logs", "ppo_kinova", datetime.now().strftime("%Y%m%d_%H%M%S"))
    os.makedirs(log_dir, exist_ok=True)

    # å¯åŠ¨ç¯å¢ƒ
    env = DummyVecEnv([make_env])

    # è®¾ç½® TensorBoard å’Œæ¨¡å‹ä¿å­˜å›è°ƒ
    checkpoint_callback = CheckpointCallback(
        save_freq=5000,
        save_path=os.path.join(log_dir, "checkpoints"),
        name_prefix="ppo_kinova"
    )

    model = PPO(
        policy="MlpPolicy",
        env=env,
        verbose=1,
        seed=SEED,
        tensorboard_log=log_dir,
    )

    print("\nğŸš€ å¼€å§‹è®­ç»ƒ PPO æ¨¡å‹...\n")
    model.learn(total_timesteps=100_000, callback=checkpoint_callback)
    print("\nâœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨:", log_dir)

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    model.save(os.path.join(log_dir, "ppo_kinova_final"))


if __name__ == "__main__":
    main()

